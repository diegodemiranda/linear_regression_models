{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyenVtrpl2VtBa984C5Jw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegodemiranda/linear_regression_models/blob/main/chicago_taxi_fare_prediction/linear_regression_chicago_taxi_fare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Linear regression - Chicago taxi fare predriction\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "With this notebook we will use a dataset to train a model to predict the fare of a taxi ride in Chicago, Illinois. ðŸš•\n",
        "\n",
        "In reality, Chicago taxi cabs use a documented formula to determine cab fares.\n",
        "For a single passenger paying cash, the fare is calculated like this:\n",
        "\n",
        "**FARE** = 2.25 * `TRIP_MILES` + 0.12 * `TRIP_MINUTES` + 3.25\n",
        "\n",
        "Typically with machine learning problems we would not know the 'correct'\n",
        "formula, but in this case we will use this knowledge to evaluate our model."
      ],
      "metadata": {
        "id": "O9NOMZ5zwHo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Setup the Environment\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "oit-sCvix_D1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dependencies\n",
        "\n",
        "The model depends on several **Python** libraries to help with data manipulation, machine learning tasks, and data visualization:\n",
        "* **NumPy** is a fundamental package for scientific computing in Python.\n",
        "* **Pandas** is a powerful library for data analysis and manipulation.\n",
        "* **Keras** is a high-level API for building and training deep learning models.\n",
        "* **Plotly** is an interactive, open-source, and browser-based graphing library for creating interactive and visually appealing plots.\n",
        "* **Seaborn** is a statistical data visualization library built on top of Matplotlib. It provides a higher-level interface for creating informative and aesthetically pleasing statistical graphics."
      ],
      "metadata": {
        "id": "emmsv1sQvBh2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxj5ep4-urZt"
      },
      "outputs": [],
      "source": [
        "#general\n",
        "import io\n",
        "\n",
        "# data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "\n",
        "# data visualization\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset\n",
        "The following code cell loads the dataset and creates a pandas DataFrame."
      ],
      "metadata": {
        "id": "M-6A7uErvECm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chicago_taxi_dataset = pd.read_csv(\"chicago_taxi_train.csv\")\n",
        "\n",
        "training_df = chicago_taxi_dataset[['TRIP_MILES', 'TRIP_SECONDS', 'FARE', 'COMPANY', 'PAYMENT_TYPE', 'TIP_RATE']]\n",
        "\n",
        "print('Read dataset completed successfully.')\n",
        "print('Total number of rows: {0}\\n\\n'.format(len(training_df.index)))\n",
        "\n",
        "training_df.head(200)"
      ],
      "metadata": {
        "id": "C-DC6VTXu8hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the dataset\n",
        "\n",
        "\n",
        "Machine learning algorithms, especially those using **gradient descent**, often benefit from normalized data. Features with widely different scales can cause the model to prioritize features with larger values, leading to suboptimal performance. Normalization brings all features to a similar scale, preventing this bias.\n",
        "\n",
        "Furthermore, normalization can help the model converge faster during training, potentially reducing the number of epochs required to achieve good results. This is because the optimizer can navigate the loss landscape more efficiently when the features are on a similar scale.\n",
        "\n",
        "To scale values before training a machine learning model, we typically use a technique called **feature scaling** where we normalize or standardize our data by calculating the minimum and maximum values (for **min-max scaling**) or the mean and standard deviation (for **z-score standardization**) from our training data. Then applying the calculated values to transform all features to a common scale, usually between 0 and 1 for min-max scaling or a mean of 0 and standard deviation of 1 for z-score scaling;\n",
        "\n",
        "We should always apply scaling to both our training and testing data using the parameters calculated from the training set only to avoid **data leakage** from the test set.\n",
        "\n",
        "For this specific case, where we are working with `TRIP_MILES` and `TRIP_SECONDS` as features, is recommended use **min-max scaling**.\n",
        "This technique scales the features to a specific range, typically between 0 and 1."
      ],
      "metadata": {
        "id": "J-dGrsPdsfaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Select features to normalize (excluding categorical ones like 'COMPANY', 'PAYMENT_TYPE')\n",
        "features_to_normalize = ['TRIP_MILES', 'TRIP_SECONDS', 'FARE', 'TIP_RATE']\n",
        "\n",
        "# Fit and transform on dataset\n",
        "chicago_taxi_dataset[features_to_normalize] = scaler.fit_transform(chicago_taxi_dataset[features_to_normalize])"
      ],
      "metadata": {
        "id": "gm1yp_QcvjCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Dataset Exploration\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UAcbzkl0yI1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View dataset statistics\n",
        "\n",
        " In this step, I will use the `DataFrame.describe` method to view **descriptive statistics** about the dataset and answer some important questions about the data."
      ],
      "metadata": {
        "id": "bUtbZj7xyxv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.describe(include='all')"
      ],
      "metadata": {
        "id": "Tu8SzOv_yOFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View correlation matrix and visualize relationships with pair plot\n",
        "\n",
        "In this step, I will use a **correlation matrix** to identify features whose values correlate well with the label. In essence, finding out how strongly related the numerical features of the taxi trip data are to each other.\n",
        "\n",
        "Correlation values have the following meanings:\n",
        "\n",
        "* **+1**: Perfect positive correlation (as one feature increases, the other increases proportionally)\n",
        "* **-1**: Perfect negative correlation (as one feature increases, the other decreases proportionally)\n",
        "* **0**: No linear correlation (no relationship between the features).\n",
        "\n",
        "In general, the higher the absolute value of a correlation value, the greater its predictive power.\n",
        "The correlation **does not imply causality**. The correlation matrix only captures **linear relationships**. If the relationship between the variables is non-linear, the correlation can be close to 0, even if there is a strong relationship."
      ],
      "metadata": {
        "id": "7tRzsuN94l42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df.corr(numeric_only = True)"
      ],
      "metadata": {
        "id": "Tltcz7uj4Oqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **pair plot** generates a grid of pairwise plots to visualize the relationship of each feature with all other features all in one place.\n",
        "Pair plots help us quickly identify **correlations, patterns, distribution and outliers**.\n",
        "This visualization helps you understand our data better and guide us next steps in the machine learning process."
      ],
      "metadata": {
        "id": "ebdn_lRsS29n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(training_df, x_vars=[\"FARE\", \"TRIP_MILES\", \"TRIP_SECONDS\"], y_vars=[\"FARE\", \"TRIP_MILES\", \"TRIP_SECONDS\"])"
      ],
      "metadata": {
        "id": "TlVo0F16S-9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Train Model\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "T8jvUXLyWjJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining plotting functions\n",
        "\n",
        "To help us visualize the results of each training run we will generate two plots at the end of each experiment:\n",
        "\n",
        "* a **scatter plot** of the features vs. the label with a line showing the output of the trained model;\n",
        "* a **loss curve**;"
      ],
      "metadata": {
        "id": "MF3DuGBJXAEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_plots(df, feature_names, label_name, model_output, sample_size=200):\n",
        "\n",
        "  random_sample = df.sample(n=sample_size).copy()\n",
        "  random_sample.reset_index()\n",
        "  weights, bias, epochs, rmse = model_output\n",
        "\n",
        "  is_2d_plot = len(feature_names) == 1\n",
        "  model_plot_type = \"scatter\" if is_2d_plot else \"surface\"\n",
        "  fig = make_subplots(rows=1, cols=2,\n",
        "                      subplot_titles=(\"Loss Curve\", \"Model Plot\"),\n",
        "                      specs=[[{\"type\": \"scatter\"}, {\"type\": model_plot_type}]])\n",
        "\n",
        "  plot_data(random_sample, feature_names, label_name, fig)\n",
        "  plot_model(random_sample, feature_names, weights, bias, fig)\n",
        "  plot_loss_curve(epochs, rmse, fig)\n",
        "\n",
        "  fig.show()\n",
        "  return\n",
        "\n",
        "def plot_loss_curve(epochs, rmse, fig):\n",
        "  curve = px.line(x=epochs, y=rmse)\n",
        "  curve.update_traces(line_color='#ff0000', line_width=3)\n",
        "\n",
        "  fig.append_trace(curve.data[0], row=1, col=1)\n",
        "  fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
        "  fig.update_yaxes(title_text=\"Root Mean Squared Error\", row=1, col=1, range=[rmse.min()*0.8, rmse.max()])\n",
        "\n",
        "  return\n",
        "\n",
        "def plot_data(df, features, label, fig):\n",
        "  if len(features) == 1:\n",
        "    scatter = px.scatter(df, x=features[0], y=label)\n",
        "  else:\n",
        "    scatter = px.scatter_3d(df, x=features[0], y=features[1], z=label)\n",
        "\n",
        "  fig.append_trace(scatter.data[0], row=1, col=2)\n",
        "  if len(features) == 1:\n",
        "    fig.update_xaxes(title_text=features[0], row=1, col=2)\n",
        "    fig.update_yaxes(title_text=label, row=1, col=2)\n",
        "  else:\n",
        "    fig.update_layout(scene1=dict(xaxis_title=features[0], yaxis_title=features[1], zaxis_title=label))\n",
        "\n",
        "  return\n",
        "\n",
        "def plot_model(df, features, weights, bias, fig):\n",
        "  df['FARE_PREDICTED'] = bias[0]\n",
        "\n",
        "  for index, feature in enumerate(features):\n",
        "    df['FARE_PREDICTED'] = df['FARE_PREDICTED'] + weights[index][0] * df[feature]\n",
        "\n",
        "  if len(features) == 1:\n",
        "    model = px.line(df, x=features[0], y='FARE_PREDICTED')\n",
        "    model.update_traces(line_color='#ff0000', line_width=3)\n",
        "  else:\n",
        "    z_name, y_name = \"FARE_PREDICTED\", features[1]\n",
        "    z = [df[z_name].min(), (df[z_name].max() - df[z_name].min()) / 2, df[z_name].max()]\n",
        "    y = [df[y_name].min(), (df[y_name].max() - df[y_name].min()) / 2, df[y_name].max()]\n",
        "    x = []\n",
        "    for i in range(len(y)):\n",
        "      x.append((z[i] - weights[1][0] * y[i] - bias[0]) / weights[0][0])\n",
        "\n",
        "    plane=pd.DataFrame({'x':x, 'y':y, 'z':[z] * 3})\n",
        "\n",
        "    light_yellow = [[0, '#89CFF0'], [1, '#FFDB58']]\n",
        "    model = go.Figure(data=go.Surface(x=plane['x'], y=plane['y'], z=plane['z'],\n",
        "                                      colorscale=light_yellow))\n",
        "\n",
        "  fig.add_trace(model.data[0], row=1, col=2)\n",
        "\n",
        "  return\n",
        "\n",
        "def model_info(feature_names, label_name, model_output):\n",
        "  weights = model_output[0]\n",
        "  bias = model_output[1]\n",
        "\n",
        "  nl = \"\\n\"\n",
        "  header = \"-\" * 80\n",
        "  banner = header + nl + \"|\" + \"MODEL INFO\".center(78) + \"|\" + nl + header\n",
        "\n",
        "  info = \"\"\n",
        "  equation = label_name + \" = \"\n",
        "\n",
        "  for index, feature in enumerate(feature_names):\n",
        "    info = info + \"Weight for feature[{}]: {:.3f}\\n\".format(feature, weights[index][0])\n",
        "    equation = equation + \"{:.3f} * {} + \".format(weights[index][0], feature)\n",
        "\n",
        "  info = info + \"Bias: {:.3f}\\n\".format(bias[0])\n",
        "  equation = equation + \"{:.3f}\\n\".format(bias[0])\n",
        "\n",
        "  return banner + nl + info + nl + equation\n",
        "\n",
        "print(\"Success: defining plotting functions complete.\")"
      ],
      "metadata": {
        "id": "fGxY5hhdW98f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining functions to build and train a model\n",
        "\n",
        "The code you need to build and train your model is in the **Define ML functions** code cell. If you would like to explore this code, expand the code cell and take a look."
      ],
      "metadata": {
        "id": "VWvX2UccXiII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(my_learning_rate, num_features):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer.\n",
        "  inputs = keras.Input(shape=(num_features,))\n",
        "  outputs = keras.layers.Dense(units=1)(inputs)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  # Compile the model topography into code that Keras can efficiently\n",
        "  # execute. Configure training to minimize the model's mean squared error.\n",
        "  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def train_model(model, df, features, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the model the feature and the label.\n",
        "  # The model will train for the specified number of epochs.\n",
        "  # input_x = df.iloc[:,1:3].values\n",
        "  # df[feature]\n",
        "  history = model.fit(x=features,\n",
        "                      y=label,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # To track the progression of training, we're going to take a snapshot\n",
        "  # of the model's root mean squared error at each epoch.\n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse\n",
        "\n",
        "\n",
        "def run_experiment(df, feature_names, label_name, learning_rate, epochs, batch_size):\n",
        "\n",
        "  print('Info: starting training experiment with features={} and label={}\\n'.format(feature_names, label_name))\n",
        "\n",
        "  num_features = len(feature_names)\n",
        "\n",
        "  features = df.loc[:, feature_names].values\n",
        "  label = df[label_name].values\n",
        "\n",
        "  model = build_model(learning_rate, num_features)\n",
        "  model_output = train_model(model, df, features, label, epochs, batch_size)\n",
        "\n",
        "  print('\\nSuccess: training experiment complete\\n')\n",
        "  print('{}'.format(model_info(feature_names, label_name, model_output)))\n",
        "  make_plots(df, feature_names, label_name, model_output)\n",
        "\n",
        "  return model\n",
        "\n",
        "print(\"Success: defining linear regression functions complete.\")"
      ],
      "metadata": {
        "id": "tG8zwUlpYAkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a model with one feature\n",
        "\n",
        "In this step we will train a model to predict the cost of the fare using a single feature. Earlier, we saw that `TRIP_MILES` (distance) correlates **most strongly** with the `FARE`, so let's start with `TRIP_MILES` as the feature for our first training run.\n",
        "\n",
        "During training, we should see the **root mean square error (RMSE)** in the output. The units for RMSE are the same as the units for the label (dollars). In other words, we can use the RMSE to determine how far off, on average, the predicted fares are in dollars from the observed values."
      ],
      "metadata": {
        "id": "dNtr6Q1HY8qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following variables are the hyperparameters.\n",
        "# We can adjust these hyperparameters to see how they impact a training run.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "# Specify the feature and the label.\n",
        "features = ['TRIP_MILES']\n",
        "label = 'FARE'\n",
        "\n",
        "model_1 = run_experiment(training_df, features, label, learning_rate, epochs, batch_size)"
      ],
      "metadata": {
        "id": "okRnhKB2Zehk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment with hyperparameters\n",
        "\n",
        "It is common with machine learning to run multiple experiments to find the best set of hyperparmeters to train our model.\n",
        "\n",
        "When the learning rate is **too high**, the loss curve bounces around and does not appear to be moving towards convergence with each iteration. Also the predicted model does not fit the data very well.\n",
        "\n",
        "When the learning rate is **too small**, it may take longer for the loss curve to converge. With a small learning rate the loss curve decreases slowly, but does not show a dramatic drop or leveling off. With a small learning rate we could increase the number of epochs so that our model will eventually converge, but it will take longer.\n",
        "\n",
        "Increasing the batch size makes each epoch run faster, but as with a smaller\n",
        "learning rate, the model does not converge with just a few epochs. So we can increasing the number of epochs and eventually we should see the\n",
        "model converge.\n",
        "\n",
        "All of this hyperparameters can be tested by making adjustments to the code above."
      ],
      "metadata": {
        "id": "ijfjQz2gavZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a model with two features\n",
        "\n",
        "The model we trained with the feature `TOTAL_MILES` demonstrates fairly strong predictive power, but is it possible to imporve the model training with two features: `TRIP_MILES` and `TRIP_SECONDS`.\n",
        "\n",
        "When training a model with more than one feature, it is important that all\n",
        "numeric values are roughly on the same scale. In this case, `TRIP_SECONDS` and\n",
        "`TRIP_MILES` do not meet this criteria. The mean value for `TRIP_MILES` is 8.3 and the mean for `TRIP_SECONDS` is 1320; that is two orders of magnitude difference.\n",
        "\n"
      ],
      "metadata": {
        "id": "GK451Il0oj6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following variables are the hyperparameters.\n",
        "# We can adjust these hyperparameters to see how they impact a training run.\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "# Specify the feature and the label.\n",
        "features = ['TRIP_MILES', 'TRIP_SECONDS']\n",
        "label = 'FARE'\n",
        "\n",
        "model_2 = run_experiment(training_df, features, label, learning_rate, epochs, batch_size)"
      ],
      "metadata": {
        "id": "FGijmAb1pOM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 - Validate Model\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tBhfTgMouQm_"
      }
    }
  ]
}